{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4_1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "HHtnD5J0f0jW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ45nv01fzD7",
        "outputId": "e9aaa92a-737f-494f-a446-ce8adb3c9131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HLJlpPtgys73C3epBTgGRON3mSvsLHC1\n",
            "To: /content/hw4-YDBX/hw4_data.zip\n",
            "100% 1.13G/1.13G [00:05<00:00, 214MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1HLJlpPtgys73C3epBTgGRON3mSvsLHC1 --output hw4_data.zip\n",
        "!unzip -q hw4_data.zip\n",
        "!rm hw4_data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Packages"
      ],
      "metadata": {
        "id": "55FwMu5if6ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import transforms\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "O7iPphsJf8ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Random Seed"
      ],
      "metadata": {
        "id": "kh-LpGSTf-Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_random_seeds(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.enabled = False"
      ],
      "metadata": {
        "id": "q56Fup8Mf-DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Class"
      ],
      "metadata": {
        "id": "EpY9KhlEgCLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mini_dataset(data.Dataset):\n",
        "    def __init__(self, data_paths, labels, transforms):\n",
        "        self.data_paths = data_paths\n",
        "        self.labels = labels\n",
        "        self.transforms = transforms\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, label = self.data_paths[index], self.labels[index]\n",
        "        img = self.transforms(Image.open(path).convert('RGB'))\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "-_zDG36Unseu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampler"
      ],
      "metadata": {
        "id": "yU-7MzMg9Zjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoriesSampler():\n",
        "\n",
        "    def __init__(self, label, n_batch, n_cls, n_per, mode, pos):\n",
        "        self.n_batch = n_batch\n",
        "        self.n_cls = n_cls\n",
        "        self.n_per = n_per\n",
        "        self.mode = mode\n",
        "        self.pos = pos\n",
        "\n",
        "        label = np.array(label)\n",
        "        self.m_ind = []\n",
        "        for i in range(max(label) + 1):\n",
        "            ind = np.argwhere(label == i).reshape(-1)\n",
        "            ind = torch.from_numpy(ind)\n",
        "            self.m_ind.append(ind)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_batch\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for i_batch in range(self.n_batch):\n",
        "            if self.mode == 'train':\n",
        "                batch = []\n",
        "                classes = torch.randperm(len(self.m_ind))[:self.n_cls]\n",
        "                for c in classes:\n",
        "                    l = self.m_ind[c]\n",
        "                    pos = torch.randperm(len(l))[:self.n_per]\n",
        "                    batch.append(l[pos])\n",
        "                batch = torch.stack(batch).t().reshape(-1)\n",
        "                yield batch\n",
        "            else:\n",
        "                yield torch.Tensor(self.pos[i_batch]).long()"
      ],
      "metadata": {
        "id": "Sx5_HB-JnQnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get DataLoader Function"
      ],
      "metadata": {
        "id": "uByoNSmP9bLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_dataset(train_path, train_batch, n_workers, train_way, train_shot, train_query):\n",
        "    \n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.Resize((84, 84)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_data_dir = os.path.join(train_path, 'train')\n",
        "    train_csv_path = os.path.join(train_path, 'train.csv')\n",
        "\n",
        "    train_paths = []\n",
        "    labels = []\n",
        "    train_labels = []\n",
        "    label_num = -1\n",
        "    with open(train_csv_path, 'r') as f:\n",
        "        for line in f.readlines()[1:]:\n",
        "            _, path, label = line.split(',')\n",
        "\n",
        "            if label not in labels:\n",
        "                labels.append(label)\n",
        "                label_num += 1\n",
        "\n",
        "            path = os.path.join(train_data_dir, path)\n",
        "            train_paths.append(path)\n",
        "            train_labels.append(label_num)\n",
        "    \n",
        "    train_set = Mini_dataset(train_paths, train_labels, train_transforms)\n",
        "    train_sampler = CategoriesSampler(train_set.labels, train_batch, train_way, train_shot + train_query, 'train', None)\n",
        "    train_loader = DataLoader(\n",
        "        train_set,\n",
        "        batch_sampler= train_sampler,\n",
        "        num_workers=n_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader\n",
        "\n",
        "def get_valid_dataset(valid_path, valid_batch, n_workers, valid_way, valid_shot, valid_query):\n",
        "    \n",
        "    valid_transforms = transforms.Compose([\n",
        "        transforms.Resize((84, 84)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    valid_data_dir = os.path.join(valid_path, 'val')\n",
        "    valid_csv_path = os.path.join(valid_path, 'val.csv')\n",
        "    valid_test_case_csv_path = os.path.join(valid_path, 'val_testcase.csv')\n",
        "\n",
        "    valid_paths = []\n",
        "    labels = []\n",
        "    valid_labels = []\n",
        "    label_num = -1\n",
        "    pos = []\n",
        "\n",
        "    with open(valid_test_case_csv_path, 'r') as f:\n",
        "        for line in f.readlines()[1:]:\n",
        "            pos.append([int(x) for x in line.split(',')[1:]])\n",
        "\n",
        "    with open(valid_csv_path, 'r') as f:\n",
        "        for line in f.readlines()[1:]:\n",
        "            _, path, label = line.split(',')\n",
        "\n",
        "            if label not in labels:\n",
        "                labels.append(label)\n",
        "                label_num += 1\n",
        "\n",
        "            path = os.path.join(valid_data_dir, path)\n",
        "            valid_paths.append(path)\n",
        "            valid_labels.append(label_num)\n",
        "    \n",
        "    valid_set = Mini_dataset(valid_paths, valid_labels, valid_transforms)\n",
        "    valid_sampler = CategoriesSampler(valid_set.labels, valid_batch, valid_way, valid_shot + valid_query, 'val', pos)\n",
        "    valid_loader = DataLoader(\n",
        "        valid_set,\n",
        "        batch_sampler=valid_sampler,\n",
        "        num_workers=n_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return valid_loader"
      ],
      "metadata": {
        "id": "VUORd_kYnb2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "zmF3fmcY9fVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Convnet(nn.Module):\n",
        "    def __init__(self, in_channels=3, hid_channels=64, out_channels=64):\n",
        "        super().__init__()\n",
        "\n",
        "        def conv_block(in_channels, out_channels):\n",
        "            bn = nn.BatchNorm2d(out_channels)\n",
        "            nn.init.uniform_(bn.weight)\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "                bn,\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2)\n",
        "            )\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            conv_block(in_channels, hid_channels),\n",
        "            conv_block(hid_channels, hid_channels),\n",
        "            conv_block(hid_channels, hid_channels),\n",
        "            conv_block(hid_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return x.view(x.size(0), -1)"
      ],
      "metadata": {
        "id": "a7BgpU8ifUx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distance Function"
      ],
      "metadata": {
        "id": "4zVrTHTc9gog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_metric(a, b):\n",
        "    n = a.shape[0]\n",
        "    m = b.shape[0]\n",
        "    a = a.unsqueeze(1).expand(n, m, -1)\n",
        "    b = b.unsqueeze(0).expand(n, m, -1)\n",
        "    logits = -((a - b)**2).sum(dim=2)\n",
        "    return logits\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    cos = nn.CosineSimilarity(dim=2, eps=1e-6)\n",
        "    n = a.shape[0]\n",
        "    m = b.shape[0]\n",
        "    a = a.unsqueeze(1).expand(n, m, -1)\n",
        "    b = b.unsqueeze(0).expand(n, m, -1)\n",
        "    logits = cos(a, b)\n",
        "    return logits\n",
        "\n",
        "def distance_func(a, b, linear):\n",
        "    # print(linear.weight)\n",
        "    a = linear(a)\n",
        "    b = linear(b)\n",
        "    logits = a * b.t()\n",
        "    return logits"
      ],
      "metadata": {
        "id": "AU43c8HofaSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "7jvW4m4q9kqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training(device, model, optimizer, train_loader, test_loader, start_epoch, n_epochs, shot, query, train_way, test_way, linear):\n",
        "    \n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(start_epoch, n_epochs):\n",
        "        \n",
        "        model.train()\n",
        "\n",
        "        train_losses = []\n",
        "        train_accs = []\n",
        "\n",
        "        for batch in tqdm(train_loader):\n",
        "        # for i, batch in enumerate(train_loader):\n",
        "            # print(linear.weight)\n",
        "            imgs, _ = batch\n",
        "            imgs = imgs.to(device)\n",
        "            \n",
        "            p = shot * train_way\n",
        "            data_shot, data_query = imgs[:p], imgs[p:]\n",
        "\n",
        "            proto = model(data_shot)\n",
        "            proto = proto.reshape(shot, train_way, -1).mean(dim=0)\n",
        "\n",
        "            labels = torch.arange(train_way).repeat(query).long().to(device)\n",
        "            # labels = labels.long().to(device)\n",
        "\n",
        "            # logits = euclidean_metric(model(data_query), proto)\n",
        "            logits = cosine_similarity(model(data_query), proto)\n",
        "            # logits = distance_func(model(data_query), proto, linear)\n",
        "            loss = loss_fn(logits, labels)\n",
        "            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            train_accs.append(acc.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        loss = sum(train_losses) / len(train_losses)\n",
        "        acc = sum(train_accs) / len(train_accs)\n",
        "        acc_std = np.std(np.array(train_accs), axis=0)\n",
        "\n",
        "        with open('./record.txt', 'a') as f:\n",
        "            f.write(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {loss:.5f}, acc = {acc:.5f}, acc_std = {acc_std:.5f}\\n\")\n",
        "        print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {loss:.5f}, acc = {acc:.5f}, acc_std = {acc_std:.5f}\\n\")"
      ],
      "metadata": {
        "id": "1q1oOprb9lNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "KqkpV8mi9nvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(device, model, valid_loader, shot, query, test_way):\n",
        "    ckpt = torch.load('./hw4_1.pt', map_location='cpu')\n",
        "    model.load_state_dict(ckpt['model'])\n",
        "    model.eval()\n",
        "    \n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader):\n",
        "        # for i, batch in enumerate(valid_loader):\n",
        "            imgs, _ = batch\n",
        "            imgs = imgs.to(device)\n",
        "            \n",
        "            p = shot * test_way\n",
        "            data_shot, data_query = imgs[:p], imgs[p:]\n",
        "\n",
        "            proto = model(data_shot)\n",
        "            proto = proto.reshape(shot, test_way, -1).mean(dim=0)\n",
        "\n",
        "            # logits = euclidean_metric(model(data_query), proto)\n",
        "            logits = cosine_similarity(model(data_query), proto)\n",
        "            # logits = distance_func(model(data_query), proto, linear)\n",
        "            preds.append(logits.argmax(dim=-1))\n",
        "    \n",
        "    firstrow = ['episode_id'] + [f'query{i}' for i in range(75)]\n",
        "    rows = [firstrow] + [[i] + pred.cpu().tolist() for i, pred in enumerate(preds)]\n",
        "    with open('./test.csv', 'w') as f:\n",
        "        mywriter = csv.writer(f, delimiter=',')\n",
        "        mywriter.writerows(rows)\n",
        "            # print(preds)\n",
        "            # input()\n",
        "    # return preds\n",
        "    # firstrow = ['episode_id'] + [f'query{i}' for i in range(75)]\n",
        "    # rows = [firstrow] + preds\n",
        "    # with open('./test.csv', 'w') as f:\n"
      ],
      "metadata": {
        "id": "ibOHnRDf12E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "seed = 0\n",
        "fix_random_seeds(seed)\n",
        "\n",
        "train_path = './hw4_data/mini'\n",
        "train_batch = 100\n",
        "train_way = 30\n",
        "\n",
        "test_path = './hw4_data/mini'\n",
        "test_batch = 600\n",
        "test_way = 5\n",
        "\n",
        "shot = 1\n",
        "query = 15\n",
        "n_workers = 0\n",
        "n_epochs = 30\n",
        "\n",
        "model = Convnet().to(device)\n",
        "linear = nn.Linear(1600, 1).to(device)\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(linear.parameters()), lr=1e-3)\n",
        "\n",
        "load_model = os.path.exists('./hw4_1.pt')\n",
        "\n",
        "start_epoch = 0\n",
        "if load_model:\n",
        "    with open('./record.txt', 'a') as f:\n",
        "        f.write('Loading model...\\n')\n",
        "    ckpt = torch.load(f'./hw4_1.pt', map_location='cpu')\n",
        "    start_epoch = ckpt['last_epoch'] + 1\n",
        "    model.load_state_dict(ckpt['model'])\n",
        "    optimizer.load_state_dict(ckpt['optim'])\n",
        "    # scheduler.load_state_dict(ckpt['scheduler'])\n",
        "else:\n",
        "    with open('./record.txt', 'w') as f:\n",
        "        f.write('')\n",
        "\n",
        "train_loader = get_training_dataset(train_path, train_batch, n_workers, train_way, shot, query)\n",
        "valid_loader = get_valid_dataset(test_path, test_batch, n_workers, test_way, shot, query)\n",
        "\n",
        "# preds = test(device, model, valid_loader, shot, query, test_way)\n",
        "training(device, model, optimizer, train_loader, valid_loader, start_epoch, n_epochs, shot, query, train_way, test_way, linear)\n",
        "\n",
        "# test_fns, test_loader = get_testing_dataset(test_repo, batch_size, n_workers, image_size)\n",
        "# if mode == 'train':\n",
        "#     n_epochs = 20\n",
        "#     train_loader = get_training_dataset('./hw3_data/p1_data/train', batch_size, n_workers, image_size)\n",
        "#     training(device, model, optimizer, train_loader, test_loader, start_epoch, n_epochs)\n",
        "# if mode == 'test':\n",
        "#     testing(device, model, test_fns, test_loader, csv_path)"
      ],
      "metadata": {
        "id": "bdw7Wjk2fcbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/tslpzq6b1mpp8v1/hw4_1.pt?dl=0 -O hw4_1.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s1iJ7fi2nfF",
        "outputId": "715a11d3-347b-4c7c-f148-8bfe0a6cb00f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-02 03:34:48--  https://www.dropbox.com/s/tslpzq6b1mpp8v1/hw4_1.pt?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:601b:18::a27d:812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/tslpzq6b1mpp8v1/hw4_1.pt [following]\n",
            "--2022-01-02 03:34:49--  https://www.dropbox.com/s/raw/tslpzq6b1mpp8v1/hw4_1.pt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucc81c4ecb0761b155a4cb302a6b.dl.dropboxusercontent.com/cd/0/inline/Bc8Fc1CLWLRtYSZQ25uD1xViCjHAT-ZM3ANliXyeACHEXJW251nqdQ7WL0_3JauOoza_KRgSE_W0aeqsdMQjPfnpxK3gJU3Ce4kIeIC0b8mVIy2Hmw1VPpmrXkR_sJzoch_7f73LhowrjQaiQgUpWYPH/file# [following]\n",
            "--2022-01-02 03:34:49--  https://ucc81c4ecb0761b155a4cb302a6b.dl.dropboxusercontent.com/cd/0/inline/Bc8Fc1CLWLRtYSZQ25uD1xViCjHAT-ZM3ANliXyeACHEXJW251nqdQ7WL0_3JauOoza_KRgSE_W0aeqsdMQjPfnpxK3gJU3Ce4kIeIC0b8mVIy2Hmw1VPpmrXkR_sJzoch_7f73LhowrjQaiQgUpWYPH/file\n",
            "Resolving ucc81c4ecb0761b155a4cb302a6b.dl.dropboxusercontent.com (ucc81c4ecb0761b155a4cb302a6b.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to ucc81c4ecb0761b155a4cb302a6b.dl.dropboxusercontent.com (ucc81c4ecb0761b155a4cb302a6b.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Bc_cSsPZ8fLJ_6FozIcQT1-VuxXWbZ2epR6wVbpUg56PBNKOHg8wJIxuZBYzoRTYvaL-EVW-Hoda27qjMQXYDs8EZLRtyuF19vNd9zKl712sVQOmHPaX7KRQMvsrwwfBhSYhDaFqoZyx93BcM7ZlLGPcug04NI82p1bKoaAyTqnqAvOyG_VsEfkKTFFRrIUWPym9KOXgWXnprDHAv0_b9Cmgp203dTy1uFQ3cei1ysOhayZEINDZQvrW5GGHNcHCIc6JluW50SQW9y-oJnkx3qreWLTydbBqz8FMxElwQ0vAzz-JiFugPUBF7EATw-rW-LtwwS8D60v0jyu-5QbnTZHZnZaFF9rppfZUK7RPi_1vo2qOtQvzNeDSUVq01DBrgCI/file [following]\n",
            "--2022-01-02 03:34:49--  https://ucc81c4ecb0761b155a4cb302a6b.dl.dropboxusercontent.com/cd/0/inline2/Bc_cSsPZ8fLJ_6FozIcQT1-VuxXWbZ2epR6wVbpUg56PBNKOHg8wJIxuZBYzoRTYvaL-EVW-Hoda27qjMQXYDs8EZLRtyuF19vNd9zKl712sVQOmHPaX7KRQMvsrwwfBhSYhDaFqoZyx93BcM7ZlLGPcug04NI82p1bKoaAyTqnqAvOyG_VsEfkKTFFRrIUWPym9KOXgWXnprDHAv0_b9Cmgp203dTy1uFQ3cei1ysOhayZEINDZQvrW5GGHNcHCIc6JluW50SQW9y-oJnkx3qreWLTydbBqz8FMxElwQ0vAzz-JiFugPUBF7EATw-rW-LtwwS8D60v0jyu-5QbnTZHZnZaFF9rppfZUK7RPi_1vo2qOtQvzNeDSUVq01DBrgCI/file\n",
            "Reusing existing connection to ucc81c4ecb0761b155a4cb302a6b.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1384427 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘hw4_1.pt’\n",
            "\n",
            "hw4_1.pt            100%[===================>]   1.32M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-01-02 03:34:50 (9.62 MB/s) - ‘hw4_1.pt’ saved [1384427/1384427]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calcuate Accuracy"
      ],
      "metadata": {
        "id": "jAsckFlj9xb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mv hw4_1_2_cos.pt hw4_1.pt"
      ],
      "metadata": {
        "id": "NnHhMqK9ebOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./hw4_1_test.py --test_csv_path './hw4_data/mini/val.csv' --test_img_repo './hw4_data/mini/val' --testcase_csv './hw4_data/mini/val_testcase.csv' --output_csv './output.csv'"
      ],
      "metadata": {
        "id": "4xP0XR5kBgmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://YDBX: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@github.com/DLCV-Fall-2021/hw4-YDBX.git\n",
        "%cd hw4-YDBX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNKa62EopXWf",
        "outputId": "5096e12a-ad54-4137-edaf-5dd73ed8633b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hw4-YDBX'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 24 (delta 6), reused 20 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n",
            "/content/hw4-YDBX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash get_dataset.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SGQCdl9pslz",
        "outputId": "f4855e67-3923-4e0b-ff36-17e8d81f9e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-03 16:26:51--  https://docs.google.com/uc?export=download&confirm=&id=1gNFhiaidM26gzXJCw1GQxeoAIeuK9X4N\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.203.138, 74.125.203.100, 74.125.203.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.203.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘hw4_data.zip’\n",
            "\n",
            "hw4_data.zip            [ <=>                ]   3.07K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-01-03 16:26:51 (38.4 MB/s) - ‘hw4_data.zip’ saved [3139]\n",
            "\n",
            "Archive:  ./hw4_data.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of ./hw4_data.zip or\n",
            "        ./hw4_data.zip.zip, and cannot find ./hw4_data.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash hw4_download.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOAxWANIpw9x",
        "outputId": "44634071-cb4d-49c0-d85d-02c30329e5f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-03 16:25:59--  https://www.dropbox.com/s/uf67z6k0s851qgx/hw4_1.pt?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.85.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.85.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/uf67z6k0s851qgx/hw4_1.pt [following]\n",
            "--2022-01-03 16:25:59--  https://www.dropbox.com/s/raw/uf67z6k0s851qgx/hw4_1.pt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb76584aaf227a38252495ce8cc.dl.dropboxusercontent.com/cd/0/inline/BdFSHzXKagh3z28MCMF8duO2YepD00Bdvu8LAUZeiglUTbOxxf4XhkwmFpBbpwCe8vAnyR54_ECzyrnOYYnPjOud2QDcuNdGYpzxLfBSbJsQlee8yTtozU13vZQ9AGSwIIRAVm6mU6LCRhkwbR-qS83I/file# [following]\n",
            "--2022-01-03 16:26:00--  https://ucb76584aaf227a38252495ce8cc.dl.dropboxusercontent.com/cd/0/inline/BdFSHzXKagh3z28MCMF8duO2YepD00Bdvu8LAUZeiglUTbOxxf4XhkwmFpBbpwCe8vAnyR54_ECzyrnOYYnPjOud2QDcuNdGYpzxLfBSbJsQlee8yTtozU13vZQ9AGSwIIRAVm6mU6LCRhkwbR-qS83I/file\n",
            "Resolving ucb76584aaf227a38252495ce8cc.dl.dropboxusercontent.com (ucb76584aaf227a38252495ce8cc.dl.dropboxusercontent.com)... 162.125.85.15, 2620:100:6030:15::a27d:500f\n",
            "Connecting to ucb76584aaf227a38252495ce8cc.dl.dropboxusercontent.com (ucb76584aaf227a38252495ce8cc.dl.dropboxusercontent.com)|162.125.85.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BdHmli7wR0dUq44ou8vKon_4AeBTnu4gTfI6oJyN46lX6P_ab0LHLPiDgZJAiw4XMU0FnPzdVosg3Fl7vt1kLU56fKhFOxKc9zQ2Nkiq6c0r4dfDsY6IF0Fy5DMTOkOpH6RiMRRqGXlwC-NY9uZ76ijQO-6h_YUsx7letFHwkC8QJwDJZTCOI_Pkbuxy8Rcr8gIvR-pXmjHmdabvywT6Pv3aq9HLkDocyviXqt2Bur2_xiYAbQu4mjvNJdHO7CzXEHa_THc13srujgb1h5KmeEXIKtla8yy4MCAeHfO_Q-JgnG9zBStO-6OXqmJ3fmNxGyt4eV0_8LpAWrJ1KODAQnkJdCaWS7rq8qAXD-NGHikHnJCIpwa9D8zVdqTr7PFoXXU/file [following]\n",
            "--2022-01-03 16:26:01--  https://ucb76584aaf227a38252495ce8cc.dl.dropboxusercontent.com/cd/0/inline2/BdHmli7wR0dUq44ou8vKon_4AeBTnu4gTfI6oJyN46lX6P_ab0LHLPiDgZJAiw4XMU0FnPzdVosg3Fl7vt1kLU56fKhFOxKc9zQ2Nkiq6c0r4dfDsY6IF0Fy5DMTOkOpH6RiMRRqGXlwC-NY9uZ76ijQO-6h_YUsx7letFHwkC8QJwDJZTCOI_Pkbuxy8Rcr8gIvR-pXmjHmdabvywT6Pv3aq9HLkDocyviXqt2Bur2_xiYAbQu4mjvNJdHO7CzXEHa_THc13srujgb1h5KmeEXIKtla8yy4MCAeHfO_Q-JgnG9zBStO-6OXqmJ3fmNxGyt4eV0_8LpAWrJ1KODAQnkJdCaWS7rq8qAXD-NGHikHnJCIpwa9D8zVdqTr7PFoXXU/file\n",
            "Reusing existing connection to ucb76584aaf227a38252495ce8cc.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1377393 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘hw4_1.pt’\n",
            "\n",
            "hw4_1.pt            100%[===================>]   1.31M  1.38MB/s    in 1.0s    \n",
            "\n",
            "2022-01-03 16:26:02 (1.38 MB/s) - ‘hw4_1.pt’ saved [1377393/1377393]\n",
            "\n",
            "--2022-01-03 16:26:02--  https://www.dropbox.com/s/1pbrd597o5haj8k/hw4_2.pt?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.85.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.85.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/1pbrd597o5haj8k/hw4_2.pt [following]\n",
            "--2022-01-03 16:26:03--  https://www.dropbox.com/s/raw/1pbrd597o5haj8k/hw4_2.pt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc761cbd35dfcae815c978eb3c49.dl.dropboxusercontent.com/cd/0/inline/BdHMApVKS1b0fJt-GLVGftdUTHN56O5qs1FRVsl53dp4Ftxwpma3SUyP3p7ZMWQsnN-KjbBvzqWIoGTpy4P0HzumE9_xWFVnqP2vTDtWXJJs29q86S9vS68WYyw0Vkniv01AiO9yf3ospMCUVjziIJac/file# [following]\n",
            "--2022-01-03 16:26:03--  https://uc761cbd35dfcae815c978eb3c49.dl.dropboxusercontent.com/cd/0/inline/BdHMApVKS1b0fJt-GLVGftdUTHN56O5qs1FRVsl53dp4Ftxwpma3SUyP3p7ZMWQsnN-KjbBvzqWIoGTpy4P0HzumE9_xWFVnqP2vTDtWXJJs29q86S9vS68WYyw0Vkniv01AiO9yf3ospMCUVjziIJac/file\n",
            "Resolving uc761cbd35dfcae815c978eb3c49.dl.dropboxusercontent.com (uc761cbd35dfcae815c978eb3c49.dl.dropboxusercontent.com)... 162.125.85.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to uc761cbd35dfcae815c978eb3c49.dl.dropboxusercontent.com (uc761cbd35dfcae815c978eb3c49.dl.dropboxusercontent.com)|162.125.85.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 284110973 (271M) [text/plain]\n",
            "Saving to: ‘hw4_2.pt’\n",
            "\n",
            "hw4_2.pt            100%[===================>] 270.95M  21.7MB/s    in 16s     \n",
            "\n",
            "2022-01-03 16:26:20 (16.9 MB/s) - ‘hw4_2.pt’ saved [284110973/284110973]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r ./hw4_data/mini/train ./hw4_data/office/train\n",
        "!rm ./hw4_data/mini/train.csv ./hw4_data/office/train.csv"
      ],
      "metadata": {
        "id": "03EuL9ZvxDo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash hw4_p1.sh './hw4_data/mini/val.csv' './hw4_data/mini/val' './hw4_data/mini/val_testcase.csv' './output.csv'"
      ],
      "metadata": {
        "id": "3l6VAHhRDR7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash hw4_p2.sh './hw4_data/office/val.csv' './hw4_data/office/val' './output2.csv'"
      ],
      "metadata": {
        "id": "Pa0WYVompKaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python eval.py './output.csv' './hw4_data/mini/val_testcase_gt.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkQmRH6xxw9n",
        "outputId": "747f956c-7c59-4858-ffe8-c7366e55e1c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 42.48 +- 0.79 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "test_a = []\n",
        "with open('./output.csv', 'r', newline='') as f:\n",
        "    rows = csv.reader(f)\n",
        "    for i, row in enumerate(rows):\n",
        "        if i != 0:\n",
        "            test_a.append(row[1:])\n",
        "test_b = []\n",
        "with open('./hw4_data/mini/val_testcase_gt.csv', 'r', newline='') as f:\n",
        "    rows = csv.reader(f)\n",
        "    for i, row in enumerate(rows):\n",
        "        if i != 0:\n",
        "            test_b.append(row[1:])\n",
        "x = np.array(test_a) == np.array(test_b)\n",
        "accs = np.mean(x, axis=1)\n",
        "print(f'mean = {accs.mean():.5f}, std = {accs.std():.5f}')\n",
        "# print((np.array(test_a) == np.array(test_b)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j9bN5ij6etD",
        "outputId": "c75332af-479e-4901-eb85-f1aab4a4c9aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean = 0.42478, std = 0.09873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "test_a = []\n",
        "with open('./output2.csv', 'r', newline='') as f:\n",
        "    rows = csv.reader(f)\n",
        "    for i, row in enumerate(rows):\n",
        "        if i != 0:\n",
        "            test_a.append(row[2])\n",
        "# print(test_a)\n",
        "# input()\n",
        "test_b = []\n",
        "with open('./hw4_data/office/val.csv', 'r', newline='') as f:\n",
        "    rows = csv.reader(f)\n",
        "    for i, row in enumerate(rows):\n",
        "        if i != 0:\n",
        "            test_b.append(row[2])\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for i in range(len(test_a)):\n",
        "    total += 1\n",
        "    if test_a[i] == test_b[i]:\n",
        "        correct += 1\n",
        "print(f'acc = {correct/total:.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDUDdK3FpQOT",
        "outputId": "acc54323-e27c-421d-c797-54cf53508352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc = 0.37438\n"
          ]
        }
      ]
    }
  ]
}